Timestamp,Age,Gender,Number of years of programming experience,Number of years of experience with Deep Learning,Deep learning libraries/frameworks used ,What type of data do you typically work with ?,How often do you think about energy efficiency when developing deep learning applications?,P1,Additional Comments,P2,Additional Comments,P3,Additional Comments,P4,Additional Comments,P5,Additional Comments,P6,Additional Comments,P7,Additional Comments,P8,Additional Comments
1/27/2022 18:57:17,25,Male,6,1-2,"Tensorflow, Pytorch, Keras","Text, Images",Ocassionally,Strongly agree,,Strongly Agree,,Agree,,Agree,,Agree,,Strongly Agree,,Strongly Agree,,Strongly agree,
1/27/2022 21:30:24,25,Male,4,1-2,"Tensorflow, Pytorch, Keras",Text,Never,Agree,,Strongly Agree,,Agree,Pruning may not work in every situation.,Strongly agree,,Agree,"However, it will be rare to use them in real life situations.",Agree,"To minimise memory footprint, a lot of developers experience is required.",Strongly Agree,Experience of a developer plays a lot of role here.,Agree,
1/27/2022 22:46:26,23,Male,5,<1,"Tensorflow, Pytorch, Keras","Text, Images",Never,Agree,,Agree,,Agree,,Agree,,Neutral,,Agree,,Neutral,,Strongly agree,
1/28/2022 8:42:32,20,Male,6,2-3,"Tensorflow, Pytorch, Keras","Text, Images",Never,Strongly agree,,Agree,,Agree,,Strongly agree,,Strongly Agree,,Strongly Agree,,Neutral,Can you find memory leaks before training ? wouldn't you still need small iterations to be run to observe the usage patterns,Agree,
1/28/2022 10:25:39,25,Male,2,1-2,"Pytorch, Keras",Images,Ocassionally,Strongly agree,"As we utilise the pre-trained knowledge of the model for the similar purposes, it is true that we save a lot on energy.",Strongly Agree,"I very often follow this process, as it keeps me on the safer side from model not being saved.",Strongly Agree,"When it comes to inferences , the requirement is that we need a achieve atleast 30 fps on the frames received from the camera usually because of the depth and complexity of the network. Hence it's a good practice to prune the network .",Agree,It depends on if the underlying hardware supports the precision of the quantised model .But I would agree that it significantly reduces the size of the model and inturn reduces the computational complexity.,Agree,,Strongly Agree,"As most of deep learning model training involves supervised training , it is unavoidable to reduce the number of memory access cycles. And also check-pointing equally takes up lot of write cycles when the training goes on  for days together.",Strongly Agree,"It usually happens when the underlying system is not able to handle the entire data at once , hence when we train deep learning models we go for stochastic gradient descent , that way we train model with batches of images at a time , hence there is a lesser risk of OOM.",Agree,
1/28/2022 12:31:43,21,Female,6,1-2,Pytorch,Images,Never,Agree,,Strongly Agree,,Strongly Agree,,Agree,,Agree,,Strongly Agree,,Agree,,Strongly agree,
1/28/2022 19:31:54,26,Male,5+,<1,Tensorflow,Images,Never,Strongly agree,,Strongly Agree,,Strongly Agree,,Neutral,,Agree,,Neutral,,Strongly Agree,,Strongly agree,
1/28/2022 20:06:29,26,Male,6,3-4,"Tensorflow, Pytorch, Keras","Text, Images, Audio",Never,Neutral,,Agree,,Neutral,,Neutral,,Disagree,,Neutral,,Neutral,,Neutral,
1/28/2022 20:56:33,27,Male,4,2-3,Pytorch,Biomedical signal,Never,Agree,In transfer learning will not give good results every time. Because original network is trained for one dataset & we are now using some different datasets.,Strongly Disagree,,Agree,,Agree,,Strongly Agree,,Agree,,Neutral,,Strongly disagree,
1/28/2022 21:43:25,21,Male,5,<1,"Tensorflow, Pytorch",Images,Ocassionally,Agree,"I agree with this statement, but it's also true that pre-trained models might not be available for all sorts of applications. For example, one can use models trained on ImageNet for image related applications, but such vast, generalized datasets might not be available for other applications, like Robotics for instance.",Agree,,Agree,,Disagree,"Quantization can make computations less expensive when making inferences and save energy during testing time, but compromising on accuracy isn't something people prefer doing on most applications. Even when that's okay, One might also prefer to test the original model along with the quantized model and compare their accuracies to measure the performance drop, which would actually increase energy consumption.",Agree,,Agree,,Agree,,Agree,
1/29/2022 7:49:16,23,Male,1,<1,Pytorch,Images,Everytime,Agree,,Agree,,Agree,,Agree,,Agree,,Neutral,,Neutral,,Strongly agree,
1/29/2022 11:12:41,33,Male,13,>5,"Tensorflow, Pytorch, Caffe, Theano, Keras",Images,Everytime,Strongly agree,,Agree,,Agree,,Agree,,Agree,,Agree,,Agree,,Agree,
1/29/2022 13:04:09,27,Male,~5,1-2,Pytorch,Images,Ocassionally,Agree,,Agree,,Neutral,,Neutral,,Agree,,Agree,,Agree,,Agree,
1/29/2022 22:18:50,20,Male,3,1-2,"Tensorflow, Pytorch, Keras","Text, Images, Audio",Ocassionally,Strongly agree,Training last few layers takes very little bit time when compared with training all the layers particularly for large network as the backpropagated weights will be updated slowly ,Strongly Agree,,Agree,,Strongly agree,,Agree,,Agree,The input size and output size of the NN is fixed as we cannot have dynamic sizes for NN input/output.And while training we can train the NN using batches in which the memory used by machine is fixed before intiating the training.When coming to no.of read/write operations it is influenced by the data cleaning/preparation pipeline bit the no.of operation by NN is also a constant.,Strongly Agree,,Strongly agree,
